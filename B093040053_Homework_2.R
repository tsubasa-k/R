library(readr)
library(ggplot2)
library(Metrics)
library(caret)
library(dplyr)
library(reshape2)

# 1.1 Split the dataset into training / testing set
data1 <- read.csv("C:\\Users\\USER\\Dataset2\\redwinequality.csv")
numOfObs = nrow(data1)
set.seed(2022)
train_idx = sample(1:numOfObs, 0.7 * numOfObs)
test_idx = setdiff(1:numOfObs, train_idx)

train = data1[train_idx,]
test = data1[test_idx,]

# Fit linear regression models
model1 = lm(quality ~ ., train) 
summary(model1)

# predict values
train_d = predict(model1, train)
test_d = predict(model1, test)

# calculate Mean Absolute Error(MAE) for both training and testing data. 
MAE(train$quality, train_d)
# train MAE 0.4914242
MAE(test$quality, test_d)
# test MAE 0.5215846


# 1.2 
# build glm models and perform forward selection
nullModel = lm(quality ~ 1, data = train) # Base/null model. We may add "forced-in" variables here
fullModel = lm(quality ~ (.)^2, data = train) # Full model. Here we consider up to pairwise interactions.

stepForward = step(nullModel, scope = list(lower = nullModel, upper = fullModel), direction = "forward", test = "F" )
summary(stepForward)

# predict values
train_f = predict(stepForward, train)
test_f = predict(stepForward, test)

MAE(train$quality, train_f)
# train MAE 0.4825258
MAE(test$quality, test_f)
# test MAE 0.5063145

# build glm models and perform backward selection
stepBackward = step(fullModel, scope = list(lower = nullModel, upper = fullModel), direction = "backward", test = "F" )
summary(stepBackward)

# No, these selections selected different varibles 


# 1.3
#select best lambda
library(Matrix)
library(glmnet)
lasso = cv.glmnet(x = as.matrix(train[, -12]), y = train[, 12])
best = lasso$lambda.min
best
# the best value of lambda is 0.0006283377

# build the model with the best lambda
lasso_model = glmnet(x = as.matrix(train[, -12]), y = train[, 12], lambda = best)

# evaluating MAE
MAE(train$quality, predict(lasso_model, as.matrix(train[, -12])))
# training MAE 0.4915345
MAE(test$quality, predict(lasso_model, as.matrix(test[, -12])))
# testing MAE 0.5216385


# 1.4
library(randomForest)
rf_model <- randomForest(quality ~ ., data = train, importance = TRUE)

# evaluating MAE
mae(train$quality, predict(rf, newdata = train))
# training MAE 0.1937637
mae(test$quality, predict(rf, newdata = test))
# testing MAE 0.4426052

# the importance of variance of random forest 
t <- importance(rf_model)
sorted_importance <- t[order(t[, 1], decreasing=TRUE), ]
sorted_importance

#                     %IncMSE IncNodePurity
#alcohol              46.76878     131.28471
#sulphates            45.49419     104.21323
#volatile.acidity     32.59074      83.85848
#density              29.88347      64.04811
#total.sulfur.dioxide 26.80415      56.80812
#fixed.acidity        23.80474      42.28034
#chlorides            22.22891      44.53495
#citric.acid          19.02732      45.17886
#free.sulfur.dioxide  18.78211      33.13405
#pH                   17.58391      37.73956
#residual.sugar       13.87315      36.29318

# the variable importance from lm
varImp(stepForward)
varImp(stepBackward)
varImp(rf_model)

# Q: Are random forest rankings different from those generated by linear models?
# A: Yes, random forest rankings are different from those generated by linear models(forwrad and backward).


#2.1 
# Split the data into training (70%) and testing (30%) datasets with set.seed(2022). 
data2 <- read.csv("C:\\Users\\USER\\Dataset2\\diabetes.csv")
data2$Outcome = as.factor(data2$Outcome)

num = nrow(data2)
set.seed(2022)
train_index <- sample(1:num, 0.7 * num)
test_index = setdiff(1:num, train_index)

train2 = data2[train_index,]
test2 = data2[test_index,]
# Fit a Logistic regression model to predict “Outcome” and report both the training and testing AUC.
train_glm <- glm(Outcome ~ ., data = train2, family = "binomial") 

# AUC
auc(train2$Outcome, predict(train_glm, newdata = train2))
# training AUC 0.8458
auc(test2$Outcome, predict(train_glm, newdata = test2))
# testing AUC 0.8127


# 2.2
library(rpart)
library(rpart.plot)
# Fit a classification tree that predict “Outcome”
cart <- rpart(Outcome ~., data = train2)
# Plot the best tree
prp(cart)
printcp(cart)
# base on the rpart we choose nsplit = 11 because it has lowest error.

# AUC
auc(train2$Outcome, predict(cart, newdata = train2)[, 2])
# training auc 0.8685
auc(test2$Outcome, predict(cart, newdata = test2)[, 2])
# testing auc 0.7564


# 2.3
rf_model2 <- randomForest(formula = Outcome ~ ., data = train2, importance=TRUE)
rf_model2

# AUC
auc(train2$Outcome, predict(rf_model2, type = "prob")[, 2])
#training auc 0.8380
auc(test2$Outcome, predict(rf_model2, newdata = test2, type = "prob")[, 2])
#testing auc 0.8024


# 2.4
t2 <- importance(rf_model2)
sorted_t2 <- t2[order(t2[, 1], decreasing=TRUE), ]
sorted_t2

#                                 0          1 MeanDecreaseAccuracy
#Glucose                  29.603971 29.7628526            39.227914
#Age                      18.017364  9.5112544            20.646496
#Pregnancies              15.296377  3.7599935            14.775307
#BMI                      10.582710 14.2838625            16.673199
#Insulin                   9.948623  0.9667963             8.062208
#DiabetesPedigreeFunction  5.448439  5.0441448             7.385554
#SkinThickness             5.212037 -0.5121396             3.897291
#BloodPressure             4.497986  0.1583536             3.542726

#                         MeanDecreaseGini
#Glucose                          60.38631
#Age                              33.64970
#Pregnancies                      21.81830
#BMI                              36.80269
#Insulin                          18.30431
#DiabetesPedigreeFunction         30.71755
#SkinThickness                    17.72073
#BloodPressure                    20.49043

# Q: Which variable you believe is the most important to predict the “Outcome”?
#    Justify your finding with your analysis result. 
# A: Base on the variable importance of random forest, "Glucose" is the most important.


# 3.1
# Split the data into training (70%) and testing (30%) datasets with set.seed(2022). 
smoke <- read.csv("C:\\Users\\USER\\Dataset2\\smoking.csv")
# Remove ID and oral, because there is only one value in oral and ID doesn't help. 
smoke <- smoke[, -c(1, 24)]

set.seed(2022)
rows = nrow(smoke)
train.index <- sample(1:rows, 0.7 * rows)
test.index = setdiff(1:rows, train.index)

train3 = smoke[train.index, ]
test3 = smoke[test.index, ]

# Fit Logistic regression models that predict “smoking”. 
glm_model = glm(smoking ~ (.)^2, data = train3, family = "binomial") 
predict(glm_model, train3)

# The accuracy of predictions on both the training and the testing datasets, given the default cutoff value 0.5
result <- predict(glm_model, train3, type = 'response')
revise_result <- ifelse(result > 0.5, 1, 0)
accuracy(revise_result, train3$smoking)
# training accuracy 0.762364

result <- predict(glm_model, test3, type = 'response')
revise_result <- ifelse(result > 0.5, 1, 0)
accuracy(revise_result, test3$smoking)
# testing accuracy 0.7515561


# 3.2
# True Positive Rates (Sensitivities) with cutoff value = 0.5
result <- predict(glm_model, train3, type = 'response')
revise_result <- ifelse(result > 0.5, 1, 0)

confusionMatrix(factor(train3$smoking), factor(revise_result))
# Sensitivity : 0.8252

library(pROC)
# True Positive Rates (Sensitivities) with different cutoff values (the “optimal” value).
roc <- roc(train3$smoking, predict(glm_model, train3, type = 'response'))
plot.roc(roc, print.thres = "best", print.thres.best.method = "youden", print.auc = T)
coords(roc, "best")
#  threshold specificity sensitivity
#1 0.3801506   0.6571556   0.8865907
# the optimal cutoff value is 0.3801506 
# Sensitivity is 0.8865907


# 3.3
# Plot the ROC curves of your models for both training and testing datasets.
train_roc <- roc(train3$smoking, predict(glm_model, train3, type = 'response'))
plot.roc(train_roc, print.thres = 0.3801506, print.auc = T)
# training auc = 0.845

test_roc <- roc(test3$smoking, predict(glm_model, test3, type = 'response'))
plot.roc(test_roc, print.thres = 0.3801506, print.auc = T)
# testing auc = 0.836













